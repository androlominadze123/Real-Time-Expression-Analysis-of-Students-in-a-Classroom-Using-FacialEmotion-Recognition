{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "import pandas as pd\n",
    "import threading\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", default = \"deploy.prototxt\",\n",
    "    help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", default=\"res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "    help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "    help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# parameters for loading data and images\n",
    "emotion_model_path = './models/emotion_model.hdf5'\n",
    "emotion_labels = get_labels('fer2013')\n",
    "\n",
    "# hyper-parameters for bounding boxes shape\n",
    "frame_window = 10\n",
    "emotion_offsets = (20, 40)\n",
    "\n",
    "# loading models\n",
    "face_cascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "\n",
    "# getting input model shapes for inference\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "# starting lists for calculating modes\n",
    "emotion_window = []\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "\n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0)\n",
    "vs.start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "detected_objects = {}\n",
    "face = {}\n",
    "\n",
    "# starting video streaming\n",
    "\n",
    "cv2.namedWindow('window_frame', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "##########################################################\n",
    "\n",
    "\n",
    "def write_to_csv():\n",
    "    with open('result.csv', 'w') as f:\n",
    "        for key in face.keys():\n",
    "            print('THESE ARE FACE KEYS:', key , type(key))\n",
    "            if key == 0:\n",
    "                f.write(\"%s,%s\"%(key, face[key]))\n",
    "            else:\n",
    "                f.write(\"$$$%s,%s\"%(key, face[key]))\n",
    "                #f.write(\"$$$%s,%s\\n\"%(key, face[key]))\n",
    "    f.close()\n",
    "\n",
    "def read_from_csv():\n",
    "    with open('result.csv', 'r') as f_read:\n",
    "        txt = f_read.read()\n",
    "        print('[COMING TXT]',txt)\n",
    "        txt = txt.split('$$$')\n",
    "        for i in txt:\n",
    "            i = i.split(',')\n",
    "        txt = pd.DataFrame(txt)\n",
    "        #txt = np.array(txt)\n",
    "        print(txt.shape)\n",
    "        print (txt)\n",
    "        #\n",
    "        \"\"\"for i in txt:\n",
    "            i = i.split(',')\n",
    "            i = pd.DataFrame(i.items())\n",
    "            print(i)\n",
    "            print(type(i))\n",
    "        print (txt[0])\"\"\"\n",
    "\n",
    "    f_read.close()\n",
    "\n",
    "\n",
    "\"\"\"def averages():\n",
    "\n",
    "    df = pd.DataFrame(face)\n",
    "    print(df)\n",
    "    column_means = df.mean(axis = 0)\n",
    "    print(column_means)\"\"\"\n",
    "\n",
    "\n",
    "def createlist(objectID):\n",
    "    if objectID in face.keys():\n",
    "        pass\n",
    "    else:\n",
    "        face[objectID] = []\n",
    "        return face[objectID]\n",
    "\n",
    "\n",
    "# FUNCTION OF RECOGNITION\n",
    "\n",
    "def recognition(arguments):\n",
    "    frame, rects, detected_objects = arguments\n",
    "    #frame, rects, emotion_labels, frame_window, emotion_offsets, emotion_classifier, emotion_window, detected_objects = arguments\n",
    "    bgr_image = frame\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = rects\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, emotion_target_size)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        \"\"\"for i in emotion_prediction:\n",
    "            for j in i:\n",
    "                j = round(float(j),4)\"\"\"\n",
    "        emotion_prediction  = np.array(emotion_prediction)\n",
    "        face[objectID].append(emotion_prediction[0])\n",
    "        #averages()\n",
    "        #print (face)\n",
    "        \n",
    "        #print('object ID:', objectID, ':', emotion_prediction)\n",
    "        #for i in range(len(emotion_labels)+1):\n",
    "        #    face[emotion_labels[i]] = 0#emotion_prediction[i]\n",
    "\n",
    "        #for i in emotion_prediction:\n",
    "            #for j in i:\n",
    "        #j = float(j)\n",
    "        #print(\"emotion prediction:\", emotion_prediction)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "\n",
    "        if len(emotion_window) > frame_window:\n",
    "            emotion_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            color = emotion_probability * np.asarray((255, 0, 0))\n",
    "        elif emotion_text == 'sad':\n",
    "            color = emotion_probability * np.asarray((0, 0, 255))\n",
    "        elif emotion_text == 'happy':\n",
    "            color = emotion_probability * np.asarray((255, 255, 0))\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = emotion_probability * np.asarray((0, 255, 255))\n",
    "        else:\n",
    "            color = emotion_probability * np.asarray((0, 255, 0))\n",
    "\n",
    "        detected_objects.setdefault(objectID, []).append(emotion_text)\n",
    "\n",
    "\n",
    "        color = color.astype(int)\n",
    "        color = color.tolist()\n",
    "\n",
    "        # draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "        draw_text(face_coordinates, rgb_image, emotion_mode, color, 0, -45, 1, 1)\n",
    "        #draw_text(face_coordinates, rgb_image, emotion_prediction, color, 0, -45, 1, 1)\n",
    "\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "\n",
    "\n",
    "# END OF THE RECOGNITION FUNCTION\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    iteration = 1\n",
    "    # read the next frame from the video stream and resize it\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "\n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the frame, pass it through the network,\n",
    "    # obtain our output predictions, and initialize the list of\n",
    "    # bounding box rectangles\n",
    "blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),\n",
    "        (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # filter out weak detections by ensuring the predicted\n",
    "        # probability is greater than a minimum threshold\n",
    "        if detections[0, 0, i, 2] > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object, then update the bounding box rectangles list\n",
    "            box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "            rects.append(box.astype(\"int\"))\n",
    "\n",
    "            # draw a bounding box surrounding the object so we can\n",
    "            # visualize it\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "    \n",
    "    # update our centroid tracker using the computed set of bounding\n",
    "    # box rectangles\n",
    "    objects = ct.update(rects)\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the\n",
    "        # object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "        \n",
    "        \n",
    "        arguments = frame, rects, detected_objects\n",
    "        id_list = createlist(objectID)\n",
    "        \n",
    "        recognition(arguments)\n",
    "###########\n",
    "\n",
    "    \"\"\"arguments = frame, rects, emotion_labels, frame_window,\\\n",
    "                emotion_offsets, emotion_classifier, emotion_window,\\\n",
    "                detected_objects\"\"\"\n",
    "        \n",
    "    \n",
    "    iteration +=1\n",
    "###########\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "print (\"HERE IS THE LAST LINE\")\n",
    "#averages()\n",
    "write_to_csv()\n",
    "read_from_csv()\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "\n",
    "#exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
